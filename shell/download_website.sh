# This command downloads the Web site www.website.org/tutorials/html/.
#
# The options are:
# --recursive: download the entire Web site.
# --domains website.org: don't follow links outside website.org.
# --no-parent: don't follow links outside the directory tutorials/html/.
# --page-requisites: get all the elements that compose the page (images, CSS and so on).
# --html-extension: save files with the .html extension.
# --convert-links: convert links so that they work locally, off-line.
# --no-clobber: don't overwrite any existing files (used in case the download is interrupted and resumed).
# -N: timestamp
# -l --level=[0...inf]
# -m --mirror: Turn on options suitable for mirroring.
# This option turns on recursion and time-stamping, sets infinite recursion depth and keeps FTP directory listings.
# It is currently equivalent to: -r -N -l inf --no-remove-listing
# --no-remove-listing: Don't remove the temporary .listing files generated by FTP retrievals.
#	--level=0 \
#	--domains $1 \

download_website() {
	wget \
	--mirror \
	--page-requisites \
	--html-extension \
	--convert-links \
	--force-directories \
	"$@"
}
