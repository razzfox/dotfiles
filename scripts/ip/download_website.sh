# This command will download an ENTIRE Web site, even if not given the root directory
# (because the site may back link to its root for navigation purposes). BEWARE!
# Use option --no-parent to prevent this
#
# The options are:
# --recursive: download the entire Web site.
# --domains website.org: don't follow links outside the listed domains.
# --no-parent: don't follow links outside the /given/directory/.
# --page-requisites: get all the elements that compose the page (images, CSS and so on).
# --html-extension: save files with the .html extension.
# --convert-links: convert links so that they work locally, off-line.
# --no-clobber: don't overwrite any existing files (used in case the download is interrupted and resumed).
# -N: timestamp
# -l --level=[0...inf]
# -m --mirror: Turn on options suitable for mirroring.
# This option turns on recursion and time-stamping, sets infinite recursion depth and keeps FTP directory listings.
# It is currently equivalent to: -r -N -l inf --no-remove-listing
# --no-remove-listing: Don't remove the temporary .listing files generated by FTP retrievals.
#	--level=0 \
#	--domains $1 \
download_website () {
	wget \
	--no-parent \
	--page-requisites \
	--html-extension \
	--convert-links \
	--force-directories \
	--mirror \
	"$@"
}

download_website "$@"
